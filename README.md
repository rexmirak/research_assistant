# Research Assistant

An intelligent pipeline for processing research papers using LLMs (Ollama or Gemini) with **dynamic LLM-driven category generation**, accurate PDF parsing, metadata extraction, multi-category relevance scoring, deduplication, and automated summarization.

## Features

- **ðŸ¤– Dynamic LLM-Driven Taxonomy**: LLM generates categories from your research topic (no hardcoded categories!)
- **ðŸ“Š Multi-Category Scoring**: Papers scored across ALL categories simultaneously for best-fit placement
- **ðŸŽ¯ Flexible LLM Support**: Use local Ollama models or Google Gemini API
- **ðŸ”§ Generic & Configurable**: Runtime topic and directory configuration (no hardcoding)
- **ðŸ“„ Accurate PDF Parsing**: PyMuPDF + OCR fallback (ocrmypdf + Tesseract) + pdfminer.six
- **ðŸ” LLM-Based Metadata Extraction**: Extract titles, authors, abstracts, years using local or cloud LLMs
- **ðŸ”„ Smart Deduplication**: Exact (hash-based) and near-duplicate (MinHash-based) detection
- **âš¡ Efficient API Usage**: 2 LLM calls per paper (metadata + classification)
- **ðŸ“ Topic-Focused Summaries**: Per-paper summaries with "how this helps your research"
- **ðŸ’¾ Resumable**: SQLite cache for embeddings and OCR outputs, index-based resume logic
- **ðŸ“¤ Multiple Outputs**: JSONL master index + CSV spreadsheet + Markdown summaries per category
- **âœ… Comprehensive Testing**: 100+ unit and integration tests with 41%+ coverage

## Pipeline Flow (8 Passes)

```mermaid
graph TD
    A[ðŸ“ Input: PDF Directory + Topic] --> B[ðŸ¤– PASS 1: LLM Taxonomy Generation]
    B -->|Generate categories from topic ONLY| C[ï¿½ PASS 2: Inventory PDFs]
    C -->|Discover all PDFs| D[ðŸ” PASS 3: Metadata + Classification]
    D -->|Extract metadata + Multi-category scoring| E{Readable?}
    E -->|No| F[ï¿½ Move to need_human_element/]
    E -->|Yes| G{Topic Relevance?}
    G -->|< threshold| H[ï¿½ Move to quarantined/]
    G -->|>= threshold| I[ðŸ“ PASS 4: Move to Best Category]
    I -->|Highest scoring category| J[ðŸ”„ PASS 5: Deduplication]
    J -->|MinHash LSH| K{Duplicate?}
    K -->|Yes| L[ï¿½ Move to repeated/]
    K -->|No| M[ðŸ“ PASS 6: Update Manifests]
    M --> N[âœï¸ PASS 7: LLM Summarization]
    N -->|Topic-focused summaries| O[ðŸ’¾ PASS 8: Generate Index]
    O --> P[ðŸ“Š index.csv]
    O --> Q[ðŸ“‹ index.jsonl]
    O --> R[ðŸ“ summaries/*.md]
    O --> S[ðŸ“œ manifests/*.json]
    O --> T[ðŸ—‚ï¸ categories.json]
    
    style B fill:#e1f5ff
    style D fill:#e1f5ff
    style N fill:#e1f5ff
    style F fill:#ffe1e1
    style H fill:#ffe1e1
    style L fill:#ffe1e1
    style P fill:#e1ffe1
    style Q fill:#e1ffe1
    style R fill:#e1ffe1
    style S fill:#e1ffe1
    style T fill:#e1ffe1
```

## Architecture

```
research_assistant/
â”œâ”€â”€ cli.py                  # Main CLI entry point (8-pass pipeline)
â”œâ”€â”€ config.py               # Configuration and settings
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ taxonomy.py         # ðŸ†• LLM-based category generation from topic
â”‚   â”œâ”€â”€ inventory.py        # Directory traversal and PDF discovery
â”‚   â”œâ”€â”€ parser.py           # PDF text extraction (PyMuPDF + OCR)
â”‚   â”œâ”€â”€ metadata.py         # LLM metadata extraction + multi-category scoring
â”‚   â”œâ”€â”€ dedup.py            # MinHash near-duplicate detection
â”‚   â”œâ”€â”€ embeddings.py       # Ollama embedding generation
â”‚   â”œâ”€â”€ summarizer.py       # Topic-focused summary generation
â”‚   â”œâ”€â”€ mover.py            # File moving with dynamic folder creation
â”‚   â”œâ”€â”€ manifest.py         # Simplified category manifest tracking
â”‚   â””â”€â”€ outputs.py          # JSONL, CSV, and Markdown generation
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ cache_manager.py    # SQLite-based caching
â”‚   â”œâ”€â”€ llm_provider.py     # Unified Ollama/Gemini interface
â”‚   â”œâ”€â”€ gemini_client.py    # Google Gemini API client
â”‚   â”œâ”€â”€ hash.py             # Content hashing utilities
â”‚   â””â”€â”€ text.py             # Text normalization and processing
â””â”€â”€ tests/                  # 100+ unit and integration tests
```

## Prerequisites

- **Python 3.10+**
- **LLM Provider** (choose one or both):
  - **Ollama** (local, free) with models:
    - `deepseek-r1:8b` (metadata extraction & classification)
    - `nomic-embed-text` (embeddings)
  - **Google Gemini API** (cloud, requires API key):
    - Set `GEMINI_API_KEY` environment variable
- **Tesseract** (for OCR): `brew install tesseract` (macOS) or `apt-get install tesseract-ocr` (Linux)

## Installation

```bash
# Clone or navigate to project
cd research_assistant

# Create virtual environment
python3 -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# Option 1: Local Ollama (recommended for privacy/offline)
ollama pull deepseek-r1:8b
ollama pull nomic-embed-text

# Option 2: Gemini API (cloud-based)
# Create .env file with your API key:
echo "GEMINI_API_KEY=your_api_key_here" > .env
```

## Quick Start

```bash
# Basic usage with Gemini (recommended)
python cli.py process \
  --root-dir /path/to/papers \
  --topic "Prompt Injection Attacks in Large Language Models" \
  --llm-provider gemini \
  --workers 2

# With Ollama (local, requires models installed)
python cli.py process \
  --root-dir /path/to/papers \
  --topic "Your research topic" \
  --llm-provider ollama \
  --workers 2

# Custom topic relevance threshold (default: 5/10)
python cli.py process \
  --root-dir /path/to/papers \
  --topic "Your research topic" \
  --min-topic-relevance 7

# Resume from interrupted run (skips analyzed papers)
python cli.py process \
  --root-dir /path/to/papers \
  --topic "Your research topic" \
  --resume

# Force regenerate categories (ignore cached taxonomy)
python cli.py process \
  --root-dir /path/to/papers \
  --topic "Your research topic" \
  --force-regenerate-categories

# Dry-run (no file moves)
python cli.py process \
  --root-dir /path/to/papers \
  --topic "Your research topic" \
  --dry-run
```

## Configuration

Runtime configuration via CLI flags or `config.yaml`:

```yaml
# config.yaml (optional)
llm_provider: gemini  # or 'ollama'

# Scoring thresholds
scoring:
  min_topic_relevance: 5  # Papers below this go to quarantined/ (1-10 scale)

# Deduplication
dedup:
  similarity_threshold: 0.95
  use_minhash: true
  num_perm: 128

# LLM providers
ollama:
  summarize_model: "deepseek-r1:8b"
  classify_model: "deepseek-r1:8b"
  embed_model: "nomic-embed-text"
  temperature: 0.1
  base_url: "http://localhost:11434"

gemini:
  api_key: null  # Set via GEMINI_API_KEY environment variable
  temperature: 0.1

# Metadata enrichment
crossref:
  enabled: true
  email: "your.email@domain.com"  # Polite pool (optional)

# File organization
move:
  enabled: true
  track_manifest: true
  create_symlinks: false

# Processing
processing:
  workers: 2  # Parallel workers (recommend 2 for API rate limits)
  batch_size: 32
```

## Dynamic Category Generation

**How it works**:

1. **LLM generates categories from topic ONLY** (no papers analyzed yet)
   - Example topic: "Prompt Injection Attacks in Large Language Models"
   - LLM generates 10-15 relevant categories with definitions
   - Cached in `outputs/categories.json` and `cache/categories.json`

2. **Multi-category scoring** for each paper:
   - Paper scored against ALL categories simultaneously (1-10 scale)
   - Returns: `topic_relevance`, `category_scores` dict, `reasoning`
   - Paper placed in highest-scoring category

3. **Topic relevance filtering**:
   - Papers with `topic_relevance < threshold` â†’ `quarantined/`
   - Configurable via `--min-topic-relevance` (default: 5/10)

**Example Categories Generated**:
```json
{
  "attack_vectors": "Papers describing methods to perform prompt injection...",
  "defense_mechanisms": "Papers proposing techniques to defend against...",
  "detection_methods": "Papers focusing on identifying attacks...",
  "robustness_evaluation": "Papers developing metrics and benchmarks..."
}
```

## Manifest System & Resume Logic

**Manifest Structure** (per category):
- Tracks all papers in this category
- Stores classification reasoning and scores
- Enables resume functionality

**Manifest Entry**:
```json
{
  "paper_id": "abc123def456...",
  "title": "Defending Against Prompt Injection Attacks",
  "path": "defense_mechanisms/smith2023.pdf",
  "content_hash": "sha256:...",
  "classification_reasoning": "Paper focuses on input validation...",
  "relevance_score": 9,
  "topic_relevance": 8,
  "analyzed": true
}
```

**Resume Logic**:
- Checks `index.jsonl` for papers with `analyzed: true`
- Skips re-processing, loads from cache
- More efficient than re-running entire pipeline

## Output Structure

```
outputs/
â”œâ”€â”€ categories.json          # ðŸ†• LLM-generated taxonomy with definitions
â”œâ”€â”€ index.jsonl              # Full machine-readable index
â”œâ”€â”€ index.csv                # Spreadsheet with all metadata
â”œâ”€â”€ summaries/
â”‚   â”œâ”€â”€ attack_vectors.md    # ðŸ†• Dynamic category names
â”‚   â”œâ”€â”€ defense_mechanisms.md
â”‚   â”œâ”€â”€ quarantined.md
â”‚   â””â”€â”€ ...
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ pipeline_YYYYMMDD_HHMMSS.log  # Detailed execution log
â””â”€â”€ manifests/
    â”œâ”€â”€ attack_vectors.manifest.json  # ðŸ†• Dynamic categories
    â”œâ”€â”€ defense_mechanisms.manifest.json
    â”œâ”€â”€ quarantined.manifest.json
    â”œâ”€â”€ repeated.manifest.json
    â””â”€â”€ need_human_element.manifest.json
```

## Index Fields (JSONL/CSV)

**New fields**:
- `paper_id`: Unique identifier (content hash)
- `title`, `authors`, `year`, `venue`, `doi`, `bibtex`
- `category`: Final category (best-fit from LLM scoring)
- `topic_relevance`: 1-10 relevance to research topic
- `category_scores`: JSON dict with scores for ALL categories
- `reasoning`: LLM explanation for categorization
- `duplicate_of`: Paper ID if duplicate
- `is_duplicate`: Boolean flag
- `path`: Current file path
- `summary_file`: Link to markdown summary
- `analyzed`: Boolean (true when processing complete)

**Removed fields** (from old system):
- `original_category` - No longer tracked (papers start in flat directory)
- `status` - Replaced by explicit category placement
- `include` - Replaced by topic_relevance threshold

## Advanced Usage

### Custom topic relevance threshold
```bash
# Stricter filtering (only highly relevant papers)
python cli.py process \
  --root-dir ./papers \
  --topic "..." \
  --min-topic-relevance 7

# More permissive (include more papers)
python cli.py process \
  --root-dir ./papers \
  --topic "..." \
  --min-topic-relevance 3
```

### Working with cached categories
```bash
# Use cached taxonomy (fast)
python cli.py process --root-dir ./papers --topic "..." --resume

# Force regenerate taxonomy (if topic changed)
python cli.py process \
  --root-dir ./papers \
  --topic "..." \
  --force-regenerate-categories
```

### Parallel processing
```bash
# More workers (caution: may hit API rate limits)
python cli.py process \
  --root-dir ./papers \
  --topic "..." \
  --workers 4

# Recommended for Gemini free tier (15 RPM limit)
python cli.py process \
  --root-dir ./papers \
  --topic "..." \
  --workers 2
```

## Troubleshooting

### OCR failing
```bash
# Verify Tesseract installation
tesseract --version

# Install additional language packs if needed
brew install tesseract-lang
```

### Ollama connection issues
```bash
# Check Ollama is running
ollama list

# Restart Ollama service
brew services restart ollama
```

## Performance Tips

- **Parallel processing**: Set `--workers 4` for multiprocessing
- **Cache warming**: Run inventory + parsing first, then scoring/summarization
- **Selective OCR**: Skip OCR for born-digital PDFs (auto-detected)
- **Batch embeddings**: Automatically batched in groups of 64

## License

MIT

# Gemini API support (optional)
# 1. Get your API key from https://aistudio.google.com/app/apikey
# 2. Add GEMINI_API_KEY to your .env file (see .env.example)

# Use Gemini as LLM provider
# python cli.py process \
#   --root-dir /path/to/papers \
#   --topic "Your research topic" \
#   --llm-provider gemini

# LLM provider selection in config.yaml
# llm_provider: ollama  # or 'gemini'
# gemini:
#   api_key: "${GEMINI_API_KEY}"
